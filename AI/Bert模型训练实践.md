# BERT æ¨¡å‹è®­ç»ƒå®è·µ

****
å¹³å°ï¼šWindows 11 + NVIDIA RTX 4060 + CUDA 12.7 + Miniconda + PyTorch + Hugging Face Transformers  

ä½¿ç”¨ Hugging Face çš„ Transformers åº“ï¼ŒåŸºäº `bert-base-uncased` æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®Œæˆä¸€ä¸ª**å¥å­è¯„åˆ†/åˆ†ç±»ä»»åŠ¡**çš„è®­ç»ƒä¸é¢„æµ‹æµç¨‹ï¼Œå¹¶ä½¿ç”¨ GPU åŠ é€Ÿè®­ç»ƒã€‚

å®Œæ•´ä»£ç åœ°å€ï¼šhttps://github.com/Doge2077/learn-bert

****

## æŠ€æœ¯ä»‹ç»

****

### BERT æ ¸å¿ƒæ€æƒ³

BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ç”±Googleåœ¨2018å¹´æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ŒåŸºäºTransformeræ¶æ„ï¼Œé€šè¿‡å¤§è§„æ¨¡æ— ç›‘ç£è¯­æ–™è®­ç»ƒï¼Œèƒ½å¤Ÿæ•æ‰æ–‡æœ¬çš„æ·±å±‚è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä»¥ä¸‹æ˜¯BERTçš„æ ¸å¿ƒåŸç†åŠå…¶å„å±‚åŠŸèƒ½çš„è¯¦ç»†è§£æï¼š

---

### **ä¸€ã€BERTçš„æ ¸å¿ƒåŸç†**

****

1. **åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡**ï¼š  
   - BERTé€šè¿‡Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰åŒæ—¶æ•æ‰æ–‡æœ¬çš„**åŒå‘ä¸Šä¸‹æ–‡å…³ç³»**ï¼Œå…‹æœäº†ä¼ ç»Ÿæ¨¡å‹ï¼ˆå¦‚LSTMï¼‰å•å‘æˆ–ç®€å•åŒå‘æ‹¼æ¥çš„å±€é™æ€§ã€‚
2. **é¢„è®­ç»ƒä»»åŠ¡**ï¼š
   - **Masked Language Model (MLM)**ï¼šéšæœºé®ç›–15%çš„è¾“å…¥è¯ï¼Œæ¨¡å‹é¢„æµ‹è¢«é®ç›–çš„è¯ï¼ˆå­¦ä¹ ä¸Šä¸‹æ–‡ä¾èµ–ï¼‰ã€‚
   - **Next Sentence Prediction (NSP)**ï¼šåˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯è¿ç»­çš„ï¼ˆå­¦ä¹ å¥å­é—´å…³ç³»ï¼‰ã€‚
3. **Transformer Encoderæ¶æ„**ï¼š
   - BERTä»…ä½¿ç”¨Transformerçš„**ç¼–ç å™¨ï¼ˆEncoderï¼‰**éƒ¨åˆ†ï¼Œç”±å¤šå±‚å †å çš„ç¼–ç å™¨ç»„æˆï¼Œæ¯å±‚åŒ…å«è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚

---

### **äºŒã€BERTçš„å±‚çº§ç»“æ„**
BERTæ¨¡å‹åˆ†ä¸º**è¾“å…¥å±‚ã€åµŒå…¥å±‚ã€å¤šå±‚ç¼–ç å™¨**ã€‚ä»¥BERT-Baseä¸ºä¾‹ï¼ˆ12å±‚ç¼–ç å™¨ï¼‰ï¼Œæ¯ä¸€å±‚çš„ä½œç”¨å¦‚ä¸‹ï¼š

#### **1. è¾“å…¥å±‚ï¼ˆInput Layerï¼‰**
- **åŠŸèƒ½**ï¼šå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„è¾“å…¥å½¢å¼ã€‚
- **è¾“å…¥æ ¼å¼**ï¼š
  - `[CLS]`ï¼šå¥é¦–æ ‡è®°ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡çš„èšåˆè¡¨ç¤ºã€‚
  - `Token Embeddings`ï¼šè¯å‘é‡ï¼ˆå¦‚ `WordPiece` åˆ†è¯åçš„è¯ï¼‰ã€‚
  - `Segment Embeddings`ï¼šåŒºåˆ†å¥å­Aå’Œå¥å­Bï¼ˆç”¨äºNSPä»»åŠ¡ï¼‰ã€‚
  - `Position Embeddings`ï¼šä½ç½®ç¼–ç ï¼Œæ ‡è®°è¯çš„ä½ç½®ä¿¡æ¯ã€‚

#### **2. åµŒå…¥å±‚ï¼ˆEmbedding Layerï¼‰**
- **åŠŸèƒ½**ï¼šå°†è¾“å…¥è½¬æ¢ä¸ºç¨ å¯†å‘é‡ã€‚
  - è¯åµŒå…¥ï¼ˆToken Embeddingsï¼‰ï¼šå°†è¯æ˜ å°„åˆ°ä½ç»´å‘é‡ã€‚
  - ä½ç½®åµŒå…¥ï¼ˆPosition Embeddingsï¼‰ï¼šç¼–ç è¯çš„ä½ç½®ä¿¡æ¯ã€‚
  - åˆ†æ®µåµŒå…¥ï¼ˆSegment Embeddingsï¼‰ï¼šåŒºåˆ†ä¸åŒå¥å­ï¼ˆå¦‚é—®ç­”ä»»åŠ¡ä¸­çš„é—®é¢˜å’Œç­”æ¡ˆï¼‰ã€‚

#### **3. ç¼–ç å™¨å±‚ï¼ˆEncoder Layersï¼‰**
æ¯å±‚ç¼–ç å™¨åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š**å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰**å’Œ**å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰**ï¼Œé€šè¿‡æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰ä¼˜åŒ–è®­ç»ƒã€‚

- **(1) å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰**
  - **åŠŸèƒ½**ï¼šæ•æ‰è¯ä¸è¯ä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚
  - **å®ç°**ï¼šå°†è¾“å…¥æ‹†åˆ†ä¸ºå¤šä¸ªå­ç©ºé—´ï¼ˆå¦‚12ä¸ªâ€œå¤´â€ï¼‰ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œæœ€åæ‹¼æ¥ç»“æœã€‚
  - **å…¬å¼**ï¼š  
  - $$
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    $$
    ï¼ˆå…¶ä¸­ $Q, K, V$ æ˜¯æŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µï¼Œ\( $d_k$ \) æ˜¯ç»´åº¦ï¼‰

- **(2) å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰**
  
  - **åŠŸèƒ½**ï¼šå¯¹è‡ªæ³¨æ„åŠ›çš„è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚
  - **ç»“æ„**ï¼šä¸¤å±‚å…¨è¿æ¥å±‚ï¼ˆå¦‚ä¸­é—´å±‚ç»´åº¦æ‰©å¤§ä¸º4å€ï¼‰ï¼Œæ¿€æ´»å‡½æ•°ä¸ºGELU/ReLUã€‚
  
- **(3) æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–**  
  
  - æ¯å±‚è¾“å‡ºå‰åº”ç”¨æ®‹å·®è¿æ¥ $( \text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x)) )$ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
  

#### **4. å„ç¼–ç å™¨å±‚çš„ç‰¹ç‚¹**
- **åº•å±‚ï¼ˆé è¿‘è¾“å…¥å±‚ï¼‰**ï¼šå­¦ä¹ åŸºç¡€è¯­æ³•ã€å±€éƒ¨ç‰¹å¾ï¼ˆå¦‚è¯æ€§ã€çŸ­è¯­ç»“æ„ï¼‰ã€‚
- **ä¸­å±‚**ï¼šæ•æ‰å¥å†…å’Œå¥é—´å…³ç³»ï¼ˆå¦‚æŒ‡ä»£æ¶ˆè§£ã€è¯­ä¹‰è§’è‰²ï¼‰ã€‚
- **é«˜å±‚**ï¼šæå–æŠ½è±¡è¯­ä¹‰ï¼ˆå¦‚æƒ…æ„Ÿå€¾å‘ã€æ–‡æœ¬ä¸»æ—¨ï¼‰ã€‚

---

### **ä¸‰ã€BERTçš„è¾“å‡º**
- **æœ€åä¸€å±‚ç¼–ç å™¨çš„è¾“å‡º**ï¼šæ¯ä¸ªè¯å¯¹åº”çš„ä¸Šä¸‹æ–‡å‘é‡ã€‚
- `[CLS]` å‘é‡ï¼šç”¨äºåˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰ï¼Œèšåˆå…¨å±€ä¿¡æ¯ã€‚
- å…¶ä»–è¯å‘é‡ï¼šç”¨äºåºåˆ—æ ‡æ³¨ï¼ˆå¦‚å‘½åå®ä½“è¯†åˆ«ï¼‰ã€é—®ç­”ç­‰ä»»åŠ¡ã€‚

****

## ç¯å¢ƒé…ç½®

****

- å®‰è£… Minicondaï¼šhttps://www.anaconda.com/docs/getting-started/miniconda/install#power-shell

- åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š

- ```shell
  conda create -n ai python=3.12 -y # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒ ai
  conda activate ai                 # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
  ```

- å®‰è£… Pytorchï¼šhttps://pytorch.org/

- ```shell
  # åœ¨å®˜ç½‘é€‰æ‹©ç›¸åº”Cudaç‰ˆæœ¬ï¼Œä»¥win11+4060ä¸ºä¾‹
  pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
  ```

- éªŒè¯å®‰è£…ï¼š

- ```shell
  python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
  ```

- è¾“å‡ºï¼š

- ```shell
  2.6.0+cu126
  True
  # å®‰è£…æˆåŠŸ
  ```

---

## é¡¹ç›®ç»“æ„

```
my_transformer_demo/
â”œâ”€â”€ config.json
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.csv
â”œâ”€â”€ train.py
â”œâ”€â”€ utils.py
```

---

## é…ç½®æ–‡ä»¶ `config.json`

```json
{
  "model_name": "bert-base-uncased",
  "max_length": 128,
  "train_batch_size": 8,
  "eval_batch_size": 8,
  "num_train_epochs": 3,
  "learning_rate": 2e-5,
  "output_dir": "./models",
  "num_labels": 5
}
```

---

## æ•°æ®é›† `data/dataset.csv`

****

```csv
sentence,score
"The weather is perfect today!",5
"This restaurant serves awful food.",1
"I'm so happy with my new phone.",5
"The concert was mediocre at best.",3
...
```

---

## å·¥å…·å‡½æ•° `utils.py`

****

```python
import pandas as pd
from datasets import Dataset

def load_dataset(path):
    df = pd.read_csv(path)
    df['label'] = df['score'] - 1  # è¯„åˆ†1~5 â†’ label 0~4
    return Dataset.from_pandas(df[['sentence', 'label']])
```

---

## è®­ç»ƒè„šæœ¬ `train.py`

```python
import json

import torch
from sklearn.metrics import accuracy_score
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)

from utils import load_dataset

print(torch.cuda.is_available())

# ========== è®¾å¤‡æ£€æµ‹ ==========
device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cuda":
    print(f"ğŸš€ ä½¿ç”¨ GPUï¼š{torch.cuda.get_device_name(0)}")
    print(f"æ˜¾å­˜å ç”¨ï¼š{torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB")
else:
    print("æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è®­ç»ƒ")

# ========== åŠ è½½é…ç½® ==========
with open("config.json") as f:
    cfg = json.load(f)

# ========== åŠ è½½æ•°æ® ==========
tokenizer = BertTokenizer.from_pretrained(cfg["model_name"])
dataset = load_dataset("data/dataset.csv")


def tokenize(example):
    return tokenizer(example["sentence"], truncation=True, max_length=cfg["max_length"])


tokenized_dataset = dataset.map(tokenize, batched=True)
tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)

# ========== åŠ è½½æ¨¡å‹ ==========
model = BertForSequenceClassification.from_pretrained(
    cfg["model_name"],
    num_labels=cfg["num_labels"]
).to(device)


# ========== è¯„ä¼°æŒ‡æ ‡ ==========
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc}


# ========== è®­ç»ƒå‚æ•° ==========
args = TrainingArguments(
    output_dir=cfg["output_dir"],
    per_device_train_batch_size=cfg["train_batch_size"],
    per_device_eval_batch_size=cfg["eval_batch_size"],
    num_train_epochs=cfg["num_train_epochs"],
    learning_rate=cfg["learning_rate"],
    save_strategy="epoch",
    logging_dir='./logs',
    report_to="none",
    eval_strategy="epoch",
    load_best_model_at_end=True,
)

# ========== æ˜¾å­˜ç›‘æ§ Hook ==========
from transformers import TrainerCallback


class PrintMemoryCallback(TrainerCallback):
    def on_epoch_begin(self, args, state, control, **kwargs):
        if torch.cuda.is_available():
            mem = torch.cuda.memory_allocated() / 1024 ** 2
            print(f"Epoch {state.epoch:.0f} å¼€å§‹ï¼Œå½“å‰æ˜¾å­˜å ç”¨ï¼š{mem:.2f} MB")


# ========== å¯åŠ¨è®­ç»ƒå™¨ ==========
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[PrintMemoryCallback()]
)

trainer.train()

# ========== ä¿å­˜æ¨¡å‹ ==========
trainer.save_model(cfg["output_dir"])
tokenizer.save_pretrained(cfg["output_dir"])
print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨ä¿å­˜æˆåŠŸï¼")
```

---

## ä½¿ç”¨æ¨¡å‹ `predict.py`

- æ¨¡å‹ä¼šä¿å­˜åˆ° `./models/`

```python
import sys
import torch
from transformers import BertTokenizer, BertForSequenceClassification

model_path = "models"
tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForSequenceClassification.from_pretrained(model_path)

def predict(sentence):
    inputs = tokenizer(sentence, return_tensors="pt", truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        score = torch.argmax(probs) + 1  # label 0~4 â†’ score 1~5
        return score.item()

if __name__ == "__main__":
    sentence = " ".join(sys.argv[1:]) or "This is a test."
    print(f"Score: {predict(sentence)}")
```

## 
