# 梯度下降法

****

梯度下降法(Gradient Descent)是机器学习中最核心的优化算法之一，用于寻找使损失函数最小化的参数值。

****

## 基本概念

****

梯度下降法的核心思想是：沿着函数梯度的反方向逐步调整参数，使函数值不断减小。

从数学上看：
- 梯度(∇f)方向是函数增长最快的方向
- 梯度的反方向(-∇f)是函数下降最快的方向

在进行模型训练时，有三个基础的概念：

- **Epoch**：使用全部数据对模型进行以此完整训练，训练轮次
- **Batch_size**：使用训练集中的小部分祥本对模型权重进行以此反向传播的参数更新，每次训练每批次样本数量
- **Iteration**：使用一个Batch数据对模型进行一次参数更新的过程

****

## 公式定义

****

参数更新公式为：
$$
W_{ij}^{new} = W_{ij}^{old} - \eta \frac{\partial E}{\partial w_{ij}}
$$
其中：
- $W_{ij}^{new}$：更新后的权重
- $W_{ij}^{old}$：更新前的权重
- $\eta$：学习率(learning rate)，控制每次更新的步长
- $\frac{\partial E}{\partial w_{ij}}$：损失函数 $E$ 对权重 $w_{ij}$ 的偏导数(梯度)

学习率 $\eta$ 的选择至关重要：

- 学习率太小：收敛速度慢，训练时间长
- 学习率太大：可能跳过最优解，甚至导致发散(无法收敛)

解决方法：

- 使用学习率衰减策略：随着训练进行逐渐减小学习率
- 使用自适应学习率算法

根据训练集大小、批量大小和批次数量的不同，梯度下降主要分为三种形式：

| 类型                              | 训练集大小 | Batch Size | Number of Batches | 说明                     |
| --------------------------------- | ---------- | ---------- | ----------------- | ------------------------ |
| BGD (Batch Gradient Descent)      | $N$        | $N$        | $1$               | 使用全部数据计算梯度     |
| SGD (Stochastic Gradient Descent) | $N$        | $1$        | $N$               | 每次使用单个样本更新     |
| Mini-Batch GD                     | $N$        | $B$        | $N/B+1$           | 折中方案，使用小批量数据 |

****

### BGD

****

更新公式：
$$
\theta_{t+1} = \theta_t - \eta \cdot \frac{1}{N} \sum_{i=1}^N \nabla_\theta J(\theta; x_i, y_i)
$$


特点：
- 每次迭代使用全部N个样本计算梯度
- 梯度估计准确，但计算成本高
- 可能陷入局部极小点

代码示例：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 生成示例数据
X = torch.randn(100, 1)  # 100个样本
y = 3 * X + 2 + 0.1 * torch.randn(100, 1)  # 线性关系 + 噪声

# 定义模型
model = nn.Linear(1, 1)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# BGD 训练
num_epochs = 100
for epoch in range(num_epochs):
    # 前向传播
    outputs = model(X)
    loss = criterion(outputs, y)
    
    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

****

### SGD

****

更新公式：
$$
\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta; x_i, y_i)
$$
其中 $i$ 随机选择

特点：
- 每次随机使用1个样本更新
- 计算快，适合在线学习
- 梯度估计噪声大，收敛不稳定

代码示例：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 生成示例数据
X = torch.randn(100, 1)  # 100个样本
y = 3 * X + 2 + 0.1 * torch.randn(100, 1)  # 线性关系 + 噪声

# 定义模型
model = nn.Linear(1, 1)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# SGD 训练
num_epochs = 100
for epoch in range(num_epochs):
    for i in range(len(X)):  # 遍历每个样本
        # 取单个样本
        x_i = X[i].unsqueeze(0)
        y_i = y[i].unsqueeze(0)
        
        # 前向传播
        outputs = model(x_i)
        loss = criterion(outputs, y_i)
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

****

### Mini-Batch GD

****

更新公式：
$$
\theta_{t+1} = \theta_t - \eta \cdot \frac{1}{B} \sum_{i=k}^{k+B-1} \nabla_\theta J(\theta; x_i, y_i)
$$
其中 $B$ 是 batch_size

特点：
- 折中方案，通常B取32-256
- 利用向量化加速计算
- 梯度估计比SGD更稳定

```python
from torch.utils.data import DataLoader, TensorDataset

# 将数据封装为 DataLoader
dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # batch_size=16

# Minibatch GD 训练
num_epochs = 100
for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:  # 遍历每个小批次
        # 前向传播
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

****

# 梯度下降优化

****

梯度下降优化算法中，可能会碰到以下情况：

- 碰到平缓区域，梯度值较小，参数优化变慢
- 碰到“鞍点”，梯度为0，参数无法优化
- 碰到局部最小值，参数不是最优

****

## 指数移动加权平均

****

### 公式定义

****

指数移动加权平均（Exponential Moving Average, EMA）是一种用于计算时间序列数据平均值的统计方法，其中最近的观测值被赋予更大的权重，而较旧的观测值权重按指数方式递减。公式如下：
$$
S_t = \begin{cases} 
Y_t, & t = 0 \\
\beta \cdot S_{t-1} + (1 - \beta) \cdot Y_t, & t > 0 
\end{cases}
$$


其中：
- $ S_t $：时间 $ t $ 的指数加权平均值。
- $ Y_t $：时间 $ t $ 的实际观测值。
- $ \beta $：平滑因子（介于0和1之间），控制权重的衰减速度。

展开 $ S_t $：

$$
\begin{align*}
S_t &= \beta S_{t-1} + (1 - \beta) Y_t \\
&= \beta (\beta S_{t-2} + (1 - \beta) Y_{t-1}) + (1 - \beta) Y_t \\
&= \beta^2 S_{t-2} + \beta (1 - \beta) Y_{t-1} + (1 - \beta) Y_t \\
&= \beta^3 S_{t-3} + \beta^2 (1 - \beta) Y_{t-2} + \beta (1 - \beta) Y_{t-1} + (1 - \beta) Y_t \\
&\ \vdots \\
&= (1 - \beta) Y_t + \beta (1 - \beta) Y_{t-1} + \beta^2 (1 - \beta) Y_{t-2} + \cdots + \beta^{t-1} (1 - \beta) Y_1 + \beta^t S_0 \\
&= (1 - \beta) \sum_{k=0}^{t-1} \beta^k Y_{t-k} + \beta^t Y_0
\end{align*}
$$
可以看到，每个历史观测值 $ Y_{t-k} $ 的权重是 $ (1 - \beta) \beta^k $，这是一个指数递减的权重：

- $ \beta $ 越大（接近1），过去的平均值对当前的影响越大，新的观测值的影响越小，因此平均值变化更平缓（更“平滑”）。
- $ \beta $ 越小（接近0），新的观测值的影响越大，平均值对近期变化更敏感，波动更大。

****

### 计算示例

****

模拟一个月（30天）的每日温度，并用指数移动平均（EMA）方法来平滑这些温度数据：

```python
import torch
import matplotlib.pyplot as plt

ELEMENT_NUMBER = 30  # 定义元素数量为30

# 1. 实际平均温度
def dayTemperature():
    # 固定随机数种子
    torch.manual_seed(0)
    # 产生30天的随机温度，范围在10左右波动
    temperature = torch.randn(size=[ELEMENT_NUMBER,]) * 10
    print(temperature)
    # 绘制平均温度
    days = torch.arange(1, ELEMENT_NUMBER + 1, 1)
    plt.plot(days, temperature, color='r', label='Daily Temperature')
    plt.scatter(days, temperature)
    plt.xlabel('Day')
    plt.ylabel('Temperature')
    plt.title('Actual Daily Temperature')
    plt.legend()
    plt.show()

# 2. 指数移动平均温度
def EMATemperature(beta=0.9):
    torch.manual_seed(0)  # 固定随机数种子
    temperature = torch.randn(size=[ELEMENT_NUMBER,]) * 10  # 产生30天的随机温度
    exp_weight_avg = []
    for idx, temp in enumerate(temperature, 1):  # 从下标1开始（实际是第0天）
        # 第一个元素的EMA值等于自身
        if idx == 1:
            exp_weight_avg.append(temp)
            continue
        # 后续元素的 EMA 值等于上一个 EMA * β + 当前气温乘以(1-β)
        new_temp = exp_weight_avg[idx - 2] * beta + (1 - beta) * temp
        exp_weight_avg.append(new_temp)
    days = torch.arange(1, ELEMENT_NUMBER + 1, 1)
    plt.plot(days, exp_weight_avg, color='b', label='EMA Temperature')
    plt.scatter(days, temperature, color='r', label='Daily Temperature')
    plt.xlabel('Day')
    plt.ylabel('Temperature')
    plt.title(f'Exponential Moving Average (beta={beta})')
    plt.legend()
    plt.show()

# 测试
dayTemperature()  # 绘制实际温度
EMATemperature()  # 绘制EMA温度（默认beta=0.9）
EMATemperature(beta=0.5)  # 绘制EMA温度（beta=0.5）
```

****

## 动量算法

****